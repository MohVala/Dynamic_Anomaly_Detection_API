# -*- coding: utf-8 -*-

"""Advance_API_Anomaly_Detection.ipynb
>>>>>>> ddf71762d4f4d638a02f9cf0a12cd1558d7aeed8

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GbJY9khwLRTEYD_eDgCrkJjwb3hpSW8e

# Main Steps:
1. Data Ingestion (Dynamic, Multi-Source API Pull)
Trigger: Scheduled (via Cloud Scheduler / Azure Data Factory / AWS EventBridge).

Process:

Pull data using APIs (with dynamic endpoint and schema handling).

Use parameterized connectors to adapt to changing data structure.

Store raw data in cloud storage (e.g., S3, Azure Blob, GCS).

üõ†Ô∏è Tools: AWS Lambda / Azure Functions / Cloud Functions + API Gateway + Retry & logging logic.

2. Data Preparation (Pipeline with Auto-Profiling)
Process:

Auto-detect schema & data types.

Clean nulls, remove duplicates.

Normalize features (Z-score, MinMax).

Generate features like time-lags, rolling means, change rates.

Use a profiling tool for dynamic reports (e.g., Great Expectations or Pandas Profiling).

üõ†Ô∏è Tools: PySpark (on EMR, Databricks, or GCP DataProc), Pandas + Scikit-Learn for preprocessing.

3. Model Training (Multi-Model with Hyperparameter Tuning)
Approach:

Train different models in parallel: Isolation Forest, AutoEncoder, One-Class SVM, XGBoost, etc.

Use time-based cross-validation for time series data.

Run hyperparameter tuning using GridSearch or Optuna.

üõ†Ô∏è Tools: AWS SageMaker / Azure ML / Vertex AI + MLflow for tracking + Dask/Ray for parallelization.

4. Best Model Selection (AutoML Logic or Manual Rules)
Evaluation:

Use metrics like F1-Score, Precision@K, ROC-AUC, Detection Rate, Latency.

Compare across datasets, time periods, or features.

Save best model + config + metrics to model registry.

üõ†Ô∏è Tools: MLflow + custom scoring logic or AutoML services if label data exists.

5. Anomaly Scoring & Probability Estimation
Inference:

For each data point, generate:

Anomaly Score (raw)

Anomaly Probability (via sigmoid/calibrated mapping)

Anomaly Label (based on threshold)

Threshold can be static, dynamic, or learned.

üõ†Ô∏è Tools: Online inference via containerized API (Flask/FastAPI) or batch scoring in Spark/Databricks.

6. Store Results (Structured & Searchable)
Store:

Combine raw + processed + model results into one dataset.

Include audit logs: model name, version, scoring timestamp.

Save as partitioned Parquet (for querying) or in cloud-native DB (BigQuery, Redshift, Synapse).

7. Push/Expose Results via API
Options:

Push via REST API to downstream systems (e.g., fraud detection engine).

Or expose a real-time anomaly detection endpoint.

Optionally publish to Kafka / PubSub / Event Hub for streaming use.

üõ†Ô∏è Tools: FastAPI + Docker + API Gateway (with auth), or push via cloud-native messaging (SNS, Event Hub, Pub/Sub).

8. add new features based on AI and searching suggestion:
    8.1 add log every where

# Imports
"""

import requests
import pandas as pd
from datetime import datetime
import uuid
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.preprocessing import MinMaxScaler

"""# 1. Data Ingestion"""

# function that ingest dataset and convert into pandas dataframe:
def ingest_api_to_dataframe(api_url):
    def flatten_json(y):
        out = {}

        def flatten(x, name=''):
            if isinstance(x, dict):
                for a in x:
                    flatten(x[a], f'{name}{a}_')
            elif isinstance(x, list):
                for i, a in enumerate(x):
                    flatten(a, f'{name}{i}_')
            else:
                out[name[:-1]] = x

        flatten(y)
        return out

    try:
        response = requests.get(api_url)
        response.raise_for_status()
        raw_json = response.json()

        # Handle list or single object
        if isinstance(raw_json, list):
            data = [flatten_json(item) for item in raw_json]
        else:
            data = [flatten_json(raw_json)]

        return pd.DataFrame(data)

    except Exception as e:
        print(f"Error: {e}")
        return pd.DataFrame()
# get the URL from user:
API_URL = input("Enter the URL of the API: ")
ingested_df = ingest_api_to_dataframe(API_URL)
ingested_df.head()
# sample dataset test:
# "https://jsonplaceholder.typicode.com/users"
# https://disease.sh/v3/covid-19/countries
# https://disease.sh/v3/covid-19/countries
# https://data.cityofnewyork.us/resource/biws-g3hs.json?$limit=10000

numeric_cols = [
    'passenger_count', 'trip_distance', 'ratecodeid', 'pulocationid', 'dolocationid',
    'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount',
    'improvement_surcharge', 'total_amount'
]

for col in numeric_cols:
    ingested_df[col] = pd.to_numeric(ingested_df[col], errors='coerce')  # converts and sets invalid parsing to NaN


"""# 2.Data Preparation

### Dataset primary Information
"""

def summarize_dataframe(df):
    print("üîç Data Summary:")
    print(f"- Shape: {df.shape}")
    print("\nüìã Column Types:")
    print(df.dtypes)

    print("\nüï≥Ô∏è Missing Values per Column:")
    print(df.isnull().sum())

    print(f"\nüîÅ Duplicate Rows: {df.duplicated().sum()}")
summarize_dataframe(ingested_df)

"""### Fill Null Values using ML Algorithm
### and remove duplications
"""

def fill_missing_with_kmeans(df, n_clusters=3):
    df_filled = df.copy()

    # ‚úÖ Remove duplicates
    df_filled.drop_duplicates(inplace=True)

    # Keep only numeric columns
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    df_numeric = df[numeric_cols]

    # Rows with no missing values for training
    df_train = df_numeric.dropna()

    if df_train.empty:
        print("‚ùå Not enough data to train KMeans (all rows have NaNs).")
        return df

    # Scale numeric data
    scaler = StandardScaler()
    scaled_train = scaler.fit_transform(df_train)

    # Fit KMeans
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(scaled_train)

    # Use scaled cluster centers for imputation
    cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)

    # Predict cluster centers for missing data
    for idx in df.index:
        row = df_numeric.loc[idx]
        if row.isnull().any():
            # Temporarily fill NaNs with column means for scaling
            temp_row = row.fillna(df_train.mean())
            temp_scaled = scaler.transform([temp_row])[0]
            cluster = kmeans.predict([temp_scaled])[0]
            cluster_center = cluster_centers[cluster]
            # Fill only missing values from cluster center
            for col in row.index[row.isnull()]:
                df_filled.at[idx, col] = cluster_center[df_numeric.columns.get_loc(col)]

    return df_filled

filled_df = fill_missing_with_kmeans(ingested_df, n_clusters=3)

"""### Features Normalization"""

def normalize_features(df):
    df_normalized = df.copy()

    numeric_cols = df.select_dtypes(include=[np.number]).columns
    df_numeric = df[numeric_cols]

    scaler = MinMaxScaler()

    df_normalized[df_numeric.columns] = scaler.fit_transform(df_numeric)

    return df_normalized

normed_df = normalize_features(filled_df)
normed_df

"""# 3. Model Training"""

#pip install optuna

import logging
from sklearn.model_selection import TimeSeriesSplit
import optuna
from sklearn.ensemble import IsolationForest
from sklearn.metrics import f1_score

import matplotlib.pyplot as plt

from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler

from sklearn.decomposition import PCA

from sklearn.metrics import silhouette_score, davies_bouldin_score

from scipy.special import expit

def get_time_series_split(X, n_splits = 5):
    tscv = TimeSeriesSplit(n_splits=n_splits)
    return [(train_index, test_index) for train_index, test_index in tscv.split(X)]

def run_all_anomaly_detectors(df, methods=['isolation_forest','kmeans'], true_anomalies = None, save_models = False):
    results={}
    data = df.loc[:,normed_df.dtypes  != 'object']
    for method in methods:
        logging.info(f"Running {method} anomaly detection")

        if method == 'isolation_forest':
            def objective(trial):
                params={
                    'n_estimators': trial.suggest_int("n_estimators", 10, 100),
                    'max_samples': trial.suggest_float("max_samples", 0.1, 1.0),
                    'contamination': trial.suggest_float("contamination", 0.01, 0.5),
                    'max_features': trial.suggest_float("max_features", 0.1, 1.0),
                    'random_state': 42,
                }
                model = IsolationForest(**params)
                preds = model.fit_predict(data)
                preds = np.where(preds == -1, 1, 0)
                if true_anomalies is not None:
                    return f1_score(true_anomalies, preds)
                else:
                    return np.mean(model.decision_function(data))

            study = optuna.create_study(direction="maximize")
            study.optimize(objective, n_trials=20)
            best_params = study.best_params


            final_model = IsolationForest(**best_params, random_state=42)
            final_model.fit(data)
            preds = np.where(final_model.predict(data) == -1, 1, 0)

            score = f1_score(true_anomalies, preds) if true_anomalies is not None else np.mean(final_model.decision_function(data))
            results[method] = {
                'model': final_model,
                'score': score,
                'params': best_params,
                "preds": preds
            }

            if save_models:
                import joblib
                joblib.dum(final_model, "isolation_forest_model.pkl")

        elif method == 'kmeans':
            best_k, best_score = -1,-1
            best_model = None

            for k in range(2,7):
                model = KMeans(n_clusters = k, random_state = 42)
                labels = model.fit_predict(data)
                sil = silhouette_score(data, labels)
                if sil>best_score:
                    best_k = k
                    best_score = sil
                    best_model = model

            distances = np.linalg.norm(data - best_model.cluster_centers_[best_model.labels_], axis=1)
            threshold = np.percentile(distances,95)
            preds = (distances > threshold).astype(int)

            score = f1_score(true_anomalies, preds) if true_anomalies is not None else best_score
            results[method] = {
                'model': best_model,
                'score': score,
                'params': {'n_clusters': best_k, "threshold":threshold},
                'preds': preds
            }

        elif method == "dbscan":
            model = DBSCAN(eps = 0.5, min_samples = 5)
            labels = model.fit_predict(data)
            preds = np.where(labels == -1,1,0)

            if true_anomalies is not None:
                score = f1_score(true_anomalies, preds)
            else:
                valid_labels = labels[labels !=-1]
                if len(set(valid_labels))>1:
                    score = silhouette_score(data[labels !=-1], valid_labels)
                else:
                    score = 0

            results[method]={
                'model': model,
                'score': score,
                'params': {'eps': 0.5, 'min_samples': 5},
                'preds': preds
            }

        else:
            logging.warning(f"Method {method} is not implemented yet.")

        logging.info(f"{method} completed with score: {results[method]['score']:.4f}")


    return results

result_dict = run_all_anomaly_detectors(normed_df.loc[:,normed_df.dtypes  != 'object'], methods=["isolation_forest", "kmeans", "dbscan"])

# To access outputs:
iso_preds = result_dict["isolation_forest"]["preds"]
kmeans_preds = result_dict["kmeans"]["preds"]
dbscan_preds = result_dict["dbscan"]["preds"]

print("Isolation Forest Predictions:", iso_preds)
print("K-Means Predictions:", kmeans_preds)
print("DBSCAN Predictions:", dbscan_preds)

<<<<<<< HEAD
"""# 4. Best Model Selection"""

def select_best_model_by_silhouette(df,result_dict):
    data = df.loc[:,df.dtypes != 'object']
    best_model = None
    best_score = -1
    best_result = None

    for method in result_dict:
        preds = result_dict[method]['preds']

        if len(set(preds)) < 2:
            print(f"[{method}] skipped - only one unique label")
            continue

        try:
            score = silhouette_score(data, preds)
            print(f"[{method}] silhouette score: {score}")

        except Exception as e:
            print(f"[{method}] Error calculating silhouette score: {e}")
            continue

        if score > best_score:
            best_score = score
            best_model = method
            best_result = result_dict[method]

    if best_model:
        print(f"\n‚úÖ Best model: {best_model} with silhouette score {best_score}")
    else:
        print("\n‚ö†Ô∏è No valid model found based on silhouette score.")

    return best_model, best_score, best_result

best_model, best_score, best_result  = select_best_model_by_silhouette(normed_df, result_dict)

print(f"Best model: {best_model} with silhouette score {best_score}")
#print(f"prediction: {best_model_result['preds']}")


"""# 5. Anomaly Scoring & Probability Estimation Inference"""


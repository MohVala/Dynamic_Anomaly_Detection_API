# -*- coding: utf-8 -*-
"""Advance_API_Anomaly_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GbJY9khwLRTEYD_eDgCrkJjwb3hpSW8e

# Main Steps:
1. Data Ingestion (Dynamic, Multi-Source API Pull)
Trigger: Scheduled (via Cloud Scheduler / Azure Data Factory / AWS EventBridge).

Process:

Pull data using APIs (with dynamic endpoint and schema handling).

Use parameterized connectors to adapt to changing data structure.

Store raw data in cloud storage (e.g., S3, Azure Blob, GCS).

ðŸ› ï¸ Tools: AWS Lambda / Azure Functions / Cloud Functions + API Gateway + Retry & logging logic.

2. Data Preparation (Pipeline with Auto-Profiling)
Process:

Auto-detect schema & data types.

Clean nulls, remove duplicates.

Normalize features (Z-score, MinMax).

Generate features like time-lags, rolling means, change rates.

Use a profiling tool for dynamic reports (e.g., Great Expectations or Pandas Profiling).

ðŸ› ï¸ Tools: PySpark (on EMR, Databricks, or GCP DataProc), Pandas + Scikit-Learn for preprocessing.

3. Model Training (Multi-Model with Hyperparameter Tuning)
Approach:

Train different models in parallel: Isolation Forest, AutoEncoder, One-Class SVM, XGBoost, etc.

Use time-based cross-validation for time series data.

Run hyperparameter tuning using GridSearch or Optuna.

ðŸ› ï¸ Tools: AWS SageMaker / Azure ML / Vertex AI + MLflow for tracking + Dask/Ray for parallelization.

4. Best Model Selection (AutoML Logic or Manual Rules)
Evaluation:

Use metrics like F1-Score, Precision@K, ROC-AUC, Detection Rate, Latency.

Compare across datasets, time periods, or features.

Save best model + config + metrics to model registry.

ðŸ› ï¸ Tools: MLflow + custom scoring logic or AutoML services if label data exists.

5. Anomaly Scoring & Probability Estimation
Inference:

For each data point, generate:

Anomaly Score (raw)

Anomaly Probability (via sigmoid/calibrated mapping)

Anomaly Label (based on threshold)

Threshold can be static, dynamic, or learned.

ðŸ› ï¸ Tools: Online inference via containerized API (Flask/FastAPI) or batch scoring in Spark/Databricks.

6. Store Results (Structured & Searchable)
Store:

Combine raw + processed + model results into one dataset.

Include audit logs: model name, version, scoring timestamp.

Save as partitioned Parquet (for querying) or in cloud-native DB (BigQuery, Redshift, Synapse).

7. Push/Expose Results via API
Options:

Push via REST API to downstream systems (e.g., fraud detection engine).

Or expose a real-time anomaly detection endpoint.

Optionally publish to Kafka / PubSub / Event Hub for streaming use.

ðŸ› ï¸ Tools: FastAPI + Docker + API Gateway (with auth), or push via cloud-native messaging (SNS, Event Hub, Pub/Sub).

8. add new features based on AI and searching suggestion:
    8.1 add log every where

# Imports
"""

import requests
import pandas as pd
from datetime import datetime
import uuid
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.preprocessing import MinMaxScaler

"""# 1. Data Ingestion"""

# function that ingest dataset and convert into pandas dataframe:
def ingest_api_to_dataframe(api_url):
    def flatten_json(y):
        out = {}

        def flatten(x, name=''):
            if isinstance(x, dict):
                for a in x:
                    flatten(x[a], f'{name}{a}_')
            elif isinstance(x, list):
                for i, a in enumerate(x):
                    flatten(a, f'{name}{i}_')
            else:
                out[name[:-1]] = x

        flatten(y)
        return out

    try:
        response = requests.get(api_url)
        response.raise_for_status()
        raw_json = response.json()

        # Handle list or single object
        if isinstance(raw_json, list):
            data = [flatten_json(item) for item in raw_json]
        else:
            data = [flatten_json(raw_json)]

        return pd.DataFrame(data)

    except Exception as e:
        print(f"Error: {e}")
        return pd.DataFrame()
# get the URL from user:
API_URL = input("Enter the URL of the API: ")
ingested_df = ingest_api_to_dataframe(API_URL)
ingested_df.head()
# sample dataset test:
# "https://jsonplaceholder.typicode.com/users"
# https://disease.sh/v3/covid-19/countries

"""# 2.Data Preparation

### Dataset primary Information
"""

def summarize_dataframe(df):
    print("ðŸ” Data Summary:")
    print(f"- Shape: {df.shape}")
    print("\nðŸ“‹ Column Types:")
    print(df.dtypes)

    print("\nðŸ•³ï¸ Missing Values per Column:")
    print(df.isnull().sum())

    print(f"\nðŸ” Duplicate Rows: {df.duplicated().sum()}")
summarize_dataframe(ingested_df)

"""### Fill Null Values using ML Algorithm
### and remove duplications
"""

def fill_missing_with_kmeans(df, n_clusters=3):
    df_filled = df.copy()

    # âœ… Remove duplicates
    df_filled.drop_duplicates(inplace=True)

    # Keep only numeric columns
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    df_numeric = df[numeric_cols]

    # Rows with no missing values for training
    df_train = df_numeric.dropna()

    if df_train.empty:
        print("âŒ Not enough data to train KMeans (all rows have NaNs).")
        return df

    # Scale numeric data
    scaler = StandardScaler()
    scaled_train = scaler.fit_transform(df_train)

    # Fit KMeans
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(scaled_train)

    # Use scaled cluster centers for imputation
    cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)

    # Predict cluster centers for missing data
    for idx in df.index:
        row = df_numeric.loc[idx]
        if row.isnull().any():
            # Temporarily fill NaNs with column means for scaling
            temp_row = row.fillna(df_train.mean())
            temp_scaled = scaler.transform([temp_row])[0]
            cluster = kmeans.predict([temp_scaled])[0]
            cluster_center = cluster_centers[cluster]
            # Fill only missing values from cluster center
            for col in row.index[row.isnull()]:
                df_filled.at[idx, col] = cluster_center[df_numeric.columns.get_loc(col)]

    return df_filled

filled_df = fill_missing_with_kmeans(ingested_df, n_clusters=3)

"""### Features Normalization"""

def normalize_features(df):
    df_normalized = df.copy()

    numeric_cols = df.select_dtypes(include=[np.number]).columns
    df_numeric = df[numeric_cols]

    scaler = MinMaxScaler()

    df_normalized[df_numeric.columns] = scaler.fit_transform(df_numeric)

    return df_normalized

normed_df = normalize_features(filled_df)
normed_df

"""# 3. Model Training"""

pip install optuna

import logging
from sklearn.model_selection import TimeSeriesSplit
import optuna
from sklearn.ensemble import IsolationForest
from sklearn.metrics import f1_score

import matplotlib.pyplot as plt

from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler

from sklearn.decomposition import PCA

from sklearn.metrics import silhouette_score, davies_bouldin_score

from scipy.special import expit

def get_time_series_split(X, n_splits = 5):
    tscv = TimeSeriesSplit(n_splits=n_splits)
    return [(train_index, test_index) for train_index, test_index in tscv.split(X)]

# Isolation Forest Modelling
def run_isolation_forest_with_tuning (X, n_splits = 5):
    def objective(trial):
        model = IsolationForest(
            n_estimators=trial.suggest_int("n_estimators", 10, 100),
            max_samples=trial.suggest_float("max_samples", 0.1, 1.0),
            contamination=trial.suggest_float("contamination", 0.01, 0.5),
            random_state=42,
        )

        score=[]

        tscv = TimeSeriesSplit(n_splits=n_splits)
        for train_index, test_index in tscv.split(X):
            X_train, X_test = X.iloc[train_index], X.iloc[test_index]
            model.fit(X_train)
            preds = model.predict(X_test)
            pres = (preds == -1).astype(int)

            #fake label: assume last 5% of test set are anomalies
            true = np.zeros_like(pres)
            true[-int(len(true) * 0.05):] = 1

            score.append(f1_score(true, pres))

        return np.mean(score)

    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=30)
    print("Best trial:",study.best_trial)

    final_model = IsolationForest(**study.best_params, random_state=42)
    final_model.fit(X)
    return final_model,study

# K-Means Modelling

def kmeans_anomaly_detection(data, n_clusters=3, contamination=0.05, random_state=42):

    df = data.copy()

    # scale data
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(df)

    # fit k-means
    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)
    kmeans.fit(scaled_data)

    # get distances to assigned cluster centers
    distances = np.linalg.norm(scaled_data - kmeans.cluster_centers_[kmeans.labels_], axis = 1)

    df['anomaly_score'] = distances

    # label top n% as anomalies
    threshold = np.percentile(distances, 100*(1-contamination))
    df['anomaly'] = (distances > threshold).astype(int)

    return df, kmeans

# visual isolation forest anomaly scores
def plot_anomaly_scroes(model, X, timestamps):
    scores = model.decision_function(X)* -1

    plt.figure(figsize=(15, 5))
    plt.plot(timestamps, scores)
    plt.title("Anomaly Scores Over Time")
    plt.xlabel("Timestamp")
    plt.ylabel("Anomaly Score")
    plt.grid(True)
    plt.show()

# visual kmeans anomaly scores

def plot_kmeans_anomalies(df, model, anomaly_col='anomaly', title='KMeans Anomaly Detection'):
    # Extract original feature columns (exclude anomaly columns)
    feature_cols = df.columns.difference([anomaly_col, 'anomaly_score'])

    # Reduce to 2D using PCA for visualization
    pca = PCA(n_components=2, random_state=42)
    components = pca.fit_transform(df[feature_cols])

    plt.figure(figsize=(10, 6))

    # Plot normal points
    plt.scatter(components[df[anomaly_col] == 0, 0], components[df[anomaly_col] == 0, 1],
                c='blue', label='Normal', alpha=0.5)

    # Plot anomalies
    plt.scatter(components[df[anomaly_col] == 1, 0], components[df[anomaly_col] == 1, 1],
                c='red', label='Anomaly', alpha=0.7)

    # Project cluster centers to PCA space
    centers_pca = pca.transform(model.cluster_centers_)
    plt.scatter(centers_pca[:, 0], centers_pca[:, 1], c='black', marker='X', s=200, label='Cluster Centers')

    plt.title(title)
    plt.xlabel('PCA Component 1')
    plt.ylabel('PCA Component 2')
    plt.legend()
    plt.show()

def run_all_anomaly_detectors(df, methods=['isolation_forest','kmeans'], true_anomalies = None, save_models = False):
    results={}
    data = df.loc[:,normed_df.dtypes  != 'object']
    for method in methods:
        logging.info(f"Running {method} anomaly detection")

        if method == 'isolation_forest':
            def objective(trial):
                params={
                    'n_estimators': trial.suggest_int("n_estimators", 10, 100),
                    'max_samples': trial.suggest_float("max_samples", 0.1, 1.0),
                    'contamination': trial.suggest_float("contamination", 0.01, 0.5),
                    'max_features': trial.suggest_float("max_features", 0.1, 1.0),
                    'random_state': 42,
                }
                model = IsolationForest(**params)
                preds = model.fit_predict(data)
                preds = np.where(preds == -1, 1, 0)
                if true_anomalies is not None:
                    return f1_score(true_anomalies, preds)
                else:
                    return np.mean(model.decision_function(data))

            study = optuna.create_study(direction="maximize")
            study.optimize(objective, n_trials=20)
            best_params = study.best_params


            final_model = IsolationForest(**best_params, random_state=42)
            final_model.fit(data)
            preds = np.where(final_model.predict(data) == -1, 1, 0)

            score = f1_score(true_anomalies, preds) if true_anomalies is not None else np.mean(final_model.decision_function(data))
            results[method] = {
                'model': final_model,
                'score': score,
                'params': best_params,
                "preds": preds
            }

            if save_models:
                import joblib
                joblib.dum(final_model, "isolation_forest_model.pkl")

        elif method == 'kmeans':
            best_k, best_score = -1,-1
            best_model = None

            for k in range(2,7):
                model = KMeans(n_clusters = k, random_state = 42)
                labels = model.fit_predict(data)
                sil = silhouette_score(data, labels)
                if sil>best_score:
                    best_k = k
                    best_score = sil
                    best_model = model

            distances = np.linalg.norm(data - best_model.cluster_centers_[best_model.labels_], axis=1)
            threshold = np.percentile(distances,95)
            preds = (distances > threshold).astype(int)

            score = f1_score(true_anomalies, preds) if true_anomalies is not None else best_score
            results[method] = {
                'model': best_model,
                'score': score,
                'params': {'n_clusters': best_k, "threshold":threshold},
                'preds': preds
            }

        elif method == "dbscan":
            model = DBSCAN(eps = 0.5, min_samples = 5)
            labels = model.fit_predict(data)
            preds = np.where(labels == -1,1,0)

            if true_anomalies is not None:
                score = f1_score(true_anomalies, preds)
            else:
                valid_labels = labels[labels !=-1]
                if len(set(valid_labels))>1:
                    score = silhouette_score(data[labels !=-1], valid_labels)
                else:
                    score = 0

            results[method]={
                'model': model,
                'score': score,
                'params': {'eps': 0.5, 'min_samples': 5},
                'preds': preds
            }

        else:
            logging.warning(f"Method {method} is not implemented yet.")

        logging.info(f"{method} completed with score: {results[method]['score']:.4f}")


    return results

result_dict = run_all_anomaly_detectors(normed_df.loc[:,normed_df.dtypes  != 'object'], methods=["isolation_forest", "kmeans", "dbscan"])

# To access outputs:
iso_preds = result_dict["isolation_forest"]["preds"]
kmeans_preds = result_dict["kmeans"]["preds"]
dbscan_preds = result_dict["dbscan"]["preds"]

print("Isolation Forest Predictions:", iso_preds)
print("K-Means Predictions:", kmeans_preds)
print("DBSCAN Predictions:", dbscan_preds)

plot_anomaly_scroes(isolation_model, normed_df.loc[:,normed_df.dtypes  != 'object'], normed_df.index)

kmeans_results, kmeans_model = kmeans_anomaly_detection(normed_df.loc[:,normed_df.dtypes  != 'object'], n_clusters=3, contamination=0.05, random_state=42)

plot_kmeans_anomalies(kmeans_results, kmeans_model)

kmeans_results

"""# 4. Best Model Selection"""

def find_best_model(data, models_dics, scoring = 'silhouette'):

    X = data.values if hasattr(data, 'values') else data
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    scores = {}

    for name, model in models_dics.items():

        # get labels depending on model type
        if hasattr(model, 'labels_'):
            labels = model.labels_
        elif hasattr(model, 'predict'):
            labels = model.predict(X_scaled)
        elif hasattr(model, 'fit_predict'):
            labels = model.fit_predict(X_scaled)
            if name.lower().find('isolation') >= 0:
                labels = np.where(labels == -1, 1, 0)
        else:
            raise ValueError(f"Model {name} does not have a 'labels_' or 'predict' attribute.")

        if len(np.unique(labels)) == 1:
            print(f"Model '{name}'has less than 2 clusters, skipping metric calculation.")
            scores[name] = None
            continue

        if scoring == 'silhouette':
            score = silhouette_score(X_scaled, labels)
        elif scoring == 'davies_bouldin':
            score = davies_bouldin_score(X_scaled, labels)
        else:
            raise ValueError(f"Unknown scoring metric: {scoring}")

        scores[name] = score

    if scoring == 'silhouette':
        best_model = max(scores, key=lambda k: scores[k] if scores[k] is not None else -np.inf)
    else: # scoring == 'davies_bouldin':
        best_model = min(scores, key=lambda k: scores[k] if scores[k] is not None else np.inf)

    return best_model, scores[best_model], scores

model_dics = {
    'Isolation Forest': isolation_model,
    'K-Means': kmeans_model
}

best_model, best_score, scores = find_best_model(normed_df.loc[:,normed_df.dtypes  != 'object'], model_dics, scoring = 'silhouette')

print(f"Best model: {best_model} with score: {best_score}")

print("all models scores:", scores)

"""# 5. Anomaly Scoring & Probability Estimation Inference"""

